// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/config.proto

package org.tensorflow.framework;

public interface ConfigProtoOrBuilder extends
    // @@protoc_insertion_point(interface_extends:tensorflow.ConfigProto)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */
  int getDeviceCountCount();
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */
  boolean containsDeviceCount(
      java.lang.String key);
  /**
   * Use {@link #getDeviceCountMap()} instead.
   */
  @java.lang.Deprecated
  java.util.Map<java.lang.String, java.lang.Integer>
  getDeviceCount();
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */
  java.util.Map<java.lang.String, java.lang.Integer>
  getDeviceCountMap();
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */

  int getDeviceCountOrDefault(
      java.lang.String key,
      int defaultValue);
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */

  int getDeviceCountOrThrow(
      java.lang.String key);

  /**
   * <pre>
   * The execution of an individual op (for some op types) can be
   * parallelized on a pool of intra_op_parallelism_threads.
   * 0 means the system picks an appropriate number.
   * </pre>
   *
   * <code>int32 intra_op_parallelism_threads = 2;</code>
   */
  int getIntraOpParallelismThreads();

  /**
   * <pre>
   * Nodes that perform blocking operations are enqueued on a pool of
   * inter_op_parallelism_threads available in each process.
   * 0 means the system picks an appropriate number.
   * Note that the first Session created in the process sets the
   * number of threads for all future sessions unless use_per_session_threads is
   * true or session_inter_op_thread_pool is configured.
   * </pre>
   *
   * <code>int32 inter_op_parallelism_threads = 5;</code>
   */
  int getInterOpParallelismThreads();

  /**
   * <pre>
   * If true, use a new set of threads for this session rather than the global
   * pool of threads. Only supported by direct sessions.
   * If false, use the global threads created by the first session, or the
   * per-session thread pools configured by session_inter_op_thread_pool.
   * This option is deprecated. The same effect can be achieved by setting
   * session_inter_op_thread_pool to have one element, whose num_threads equals
   * inter_op_parallelism_threads.
   * </pre>
   *
   * <code>bool use_per_session_threads = 9;</code>
   */
  boolean getUsePerSessionThreads();

  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future. The intended use is for when some session invocations need
   * to run in a background pool limited to a small number of threads.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * If a pool's num_threads is 0, then inter_op_parallelism_threads is used.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  java.util.List<org.tensorflow.framework.ThreadPoolOptionProto> 
      getSessionInterOpThreadPoolList();
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future. The intended use is for when some session invocations need
   * to run in a background pool limited to a small number of threads.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * If a pool's num_threads is 0, then inter_op_parallelism_threads is used.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  org.tensorflow.framework.ThreadPoolOptionProto getSessionInterOpThreadPool(int index);
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future. The intended use is for when some session invocations need
   * to run in a background pool limited to a small number of threads.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * If a pool's num_threads is 0, then inter_op_parallelism_threads is used.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  int getSessionInterOpThreadPoolCount();
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future. The intended use is for when some session invocations need
   * to run in a background pool limited to a small number of threads.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * If a pool's num_threads is 0, then inter_op_parallelism_threads is used.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  java.util.List<? extends org.tensorflow.framework.ThreadPoolOptionProtoOrBuilder> 
      getSessionInterOpThreadPoolOrBuilderList();
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future. The intended use is for when some session invocations need
   * to run in a background pool limited to a small number of threads.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * If a pool's num_threads is 0, then inter_op_parallelism_threads is used.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  org.tensorflow.framework.ThreadPoolOptionProtoOrBuilder getSessionInterOpThreadPoolOrBuilder(
      int index);

  /**
   * <pre>
   * Assignment of Nodes to Devices is recomputed every placement_period
   * steps until the system warms up (at which point the recomputation
   * typically slows down automatically).
   * </pre>
   *
   * <code>int32 placement_period = 3;</code>
   */
  int getPlacementPeriod();

  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  java.util.List<java.lang.String>
      getDeviceFiltersList();
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  int getDeviceFiltersCount();
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  java.lang.String getDeviceFilters(int index);
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  com.google.protobuf.ByteString
      getDeviceFiltersBytes(int index);

  /**
   * <pre>
   * Options that apply to all GPUs.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
   */
  boolean hasGpuOptions();
  /**
   * <pre>
   * Options that apply to all GPUs.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
   */
  org.tensorflow.framework.GPUOptions getGpuOptions();
  /**
   * <pre>
   * Options that apply to all GPUs.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
   */
  org.tensorflow.framework.GPUOptionsOrBuilder getGpuOptionsOrBuilder();

  /**
   * <pre>
   * Whether soft placement is allowed. If allow_soft_placement is true,
   * an op will be placed on CPU if
   *   1. there's no GPU implementation for the OP
   * or
   *   2. no GPU devices are known or registered
   * or
   *   3. need to co-locate with reftype input(s) which are from CPU.
   * </pre>
   *
   * <code>bool allow_soft_placement = 7;</code>
   */
  boolean getAllowSoftPlacement();

  /**
   * <pre>
   * Whether device placements should be logged.
   * </pre>
   *
   * <code>bool log_device_placement = 8;</code>
   */
  boolean getLogDevicePlacement();

  /**
   * <pre>
   * Options that apply to all graphs.
   * </pre>
   *
   * <code>.tensorflow.GraphOptions graph_options = 10;</code>
   */
  boolean hasGraphOptions();
  /**
   * <pre>
   * Options that apply to all graphs.
   * </pre>
   *
   * <code>.tensorflow.GraphOptions graph_options = 10;</code>
   */
  org.tensorflow.framework.GraphOptions getGraphOptions();
  /**
   * <pre>
   * Options that apply to all graphs.
   * </pre>
   *
   * <code>.tensorflow.GraphOptions graph_options = 10;</code>
   */
  org.tensorflow.framework.GraphOptionsOrBuilder getGraphOptionsOrBuilder();

  /**
   * <pre>
   * Global timeout for all blocking operations in this session.  If non-zero,
   * and not overridden on a per-operation basis, this value will be used as the
   * deadline for all blocking operations.
   * </pre>
   *
   * <code>int64 operation_timeout_in_ms = 11;</code>
   */
  long getOperationTimeoutInMs();

  /**
   * <pre>
   * Options that apply when this session uses the distributed runtime.
   * </pre>
   *
   * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
   */
  boolean hasRpcOptions();
  /**
   * <pre>
   * Options that apply when this session uses the distributed runtime.
   * </pre>
   *
   * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
   */
  org.tensorflow.framework.RPCOptions getRpcOptions();
  /**
   * <pre>
   * Options that apply when this session uses the distributed runtime.
   * </pre>
   *
   * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
   */
  org.tensorflow.framework.RPCOptionsOrBuilder getRpcOptionsOrBuilder();

  /**
   * <pre>
   * Optional list of all workers to use in this session.
   * </pre>
   *
   * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
   */
  boolean hasClusterDef();
  /**
   * <pre>
   * Optional list of all workers to use in this session.
   * </pre>
   *
   * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
   */
  org.tensorflow.distruntime.ClusterDef getClusterDef();
  /**
   * <pre>
   * Optional list of all workers to use in this session.
   * </pre>
   *
   * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
   */
  org.tensorflow.distruntime.ClusterDefOrBuilder getClusterDefOrBuilder();
}
